Hey everyone, here's the script shell for our presentation. I've ordered it in what I think would be a "good" order to present, but this is not hard and fast; if you think it makes sense to go in a different order, fire away. Thanks.

Remee - Database / Project Demo / Temp file storage (S3)
Hi, our group is Oh My Py. Our team consists of Luke, Martin, Xinxin, Zihan and myself Remee. A quick summary of our project is that it is a streaming facial recognition application in which the user will be able to control their media experience. We are going to go straight into the demo, this is our landing page, the user has options to select their own media file to stream. We are currently working on that feature so I will show you an example, on the play media. Here we are using s3 to stream live videos as they play our algorithm looks for faces. The next feature is learning a face, here you can select an image give it a name and the image vector will be entered into our database. Last feature is the ability to view the data from our mysql database, in the future we hope to have the ability to remove or rename the vectors to accomodate for any errors. I'll pass us on to Martin to speak about the project state

Martin Lasprilla - Overall state / Some of the Eigen math
Our project at the moment is mostly developed on the front end, including menu options that include video stream, face recognition, data produced, and other info. Although all the features that the end product will have are listed on the application, some features are not fully developed. Our team is still working on the algorithms and math involved in order to determine if a face should be recognized or not. Also, our team is working on connecting the data that is produced by the video stream. The programs currently being built will determine if data should be placed in the database or if it is an irrelevant face. However, playing media and selecting media is currently available.
Zihan Shu - Flask / front end


Xinxin Wu - OpenCV (temp files; handling files in memory) / TV_Watcher / StreamController
For displaying results, we used this page for displaying the face detection results.
** (Show a video in Play Media) **
The mechanism in it is we used openCV to read a video file frame by frame. Once we got a frame in this video, we would process it with the face detection algorithm, which will detect face and draw a rectangle on the face in that frame. After the algorithm finished drawing the box, we would convert this frame which has the 'face box' to a jpeg. After that, we send this jpeg file to the front end. As we processed the video frame by frame continuously, the front end would display a image stream. So, reading videos and sending frames are being processed simutaneously. For controlling the image stream, we are still working on it. We are expecting that the stream controller could control this image stream like a video. Moreover, we would like to add sound track to this image stream. So that's the key point in handling video, I will pass us on to Luke to speak about the FaceSpace and FaceProcessor. 

Luke - FaceSpace / FaceProcessor classes

The FaceProcessor class will be a primary controller of data flowing through the system, in effect, providing a centralized manager for the frontend and backend to communicate, as well as for the major technical processing step embodied by the FaceSpace class to communicate through.

** (Run script 'python EigenScreener.py') **

The DataConnector class will provide an interface to the database store of images. Currently it functions using the file system, though the functions are callable from other classes, so development can continue with the interface as designed, even though the implementation details will change. The timeline for the completion of the database is ______, and is being tracked in GitHub.

Finally, the FaceSpace class is where the 'heavy' lifting will take place. Upon instantiation, the class will build a Principal Components/Singular Value Decomposition of the face data present in the data store. As it is fed image vectors from the FaceProcessor, it projects them onto FaceSpace and determines the proximity to other average vectors in the space, sorting them in ascending order and placing them in order of most similar to least similar. Vectors that are in close proximity to known faces are "identified" and those that aren't are flagged and inserted into the database for future processing.
